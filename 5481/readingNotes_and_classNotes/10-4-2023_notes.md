**Class Notes**

- Viterbi alg. for decoding

- HMM

- [M]odel and [D]ata
- Bayes'
    - P(M|D) = P(D|M)P(M) / P(D)
        - more interested in the likelihood rather than the priors

- CG islands can be modeled after the "Fair Bet Casino" (H/T game)
    - Fair coin and Biased coin

- sigma: set of emission characters
    {A, C, G, T}
- Q: set of hidden states, each emitting symbols from sigma
    {CG+, CG-}
- A = (a_kl) a |Q| x |Q| matrix of probability changing from state k to state l
- E = (e_k(b)): a |Q| x |Sigma| matrix of probability of emitting symbol b while being in state k

**Hidden Paths**
- A path pi = pi_1 ... pi_n in the HMM is defined as a seq of states
- P(x | pi): prob. that a seq x was generated by the path pi

**Viterbi**
- Finds the path that maximizes P(x|pi) among all possible paths
- O(n|Q|^2) time
- worst case space Q^n

- state = max(emission * a * previous state)
    - .4 * .7 * .30 == .0840 <-- winner
    - .4 * .4 * .04 == .0064

- after calculating each state, traceback

- A profile hmm is a probabilistic rep. of mult. alignment
    - 3 states: [match insert delete] states that capture how a new seq. should align against the MSA