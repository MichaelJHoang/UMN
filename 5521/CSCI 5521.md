### CSCI 5521

##### Lecture 5

-   Bernoulli Distribution
    -   Expectation value $E[X]$ ==> weighted mean
-   Bayes' Rule
    -   $P(C|x)$ = $\frac{P(C)p(x|C)}{p(x)}$
        -   $P(C|x)$ = posterior
        -   $P(C)$ = prior
            -   Independent of the data
            -   Thing we know before we observe the data
            -   The "given", or prior information
        -   $p(x|C)$ = likelihood
            -   The part where given a certain outcome, what is the probability of observing said outcome?
        -   p(x) = evidence
            -   $\sum_i{P(x|C_i)P(C_i)}$
            -   sums up all possible categories
            -   used for normalization purposes
        -   $p(x) = p(x|C)P(C=1) + p(x|C=0)P(C=0)$
        -   ***For K>2 classes***
            -   $P(C_i | x) = \frac{p(x|C_i)P(C_i)}{\sum_{k=1}^{K}{p(x|C_k)P(C_k)}}$
            
- Parametric Methods
    - Assumes that the sample is drawn from some distribution that obeys a known model (e.g. Bernoulli, Gaussian, etc.)
    - The model is defined up to a number of params
    - Learning is to fit the model with the best parameters to the data

- Parametric Estimation
    - $X = \{x^t\}_t$ where $x^t ~ p(x)$ *squiggly symbol b/t the 2 terms*

- Maximum Likelihood Estimation
    - Likelihood of $\theta$ given the sample X
        - $I(\theta | X) = p(X | \theta) = \prod_{t} p(x^t | \theta)$
    - Log likelihood
        - $L(\theta | X) = \log{l(\theta | X)} = \sum_{t} \log{p(x^t|\theta)}$
        - monotonic function
        - commonly used among researchers
    - Maximum likelihood estimator
        - $\theta^* = argmax_{\theta}L(\theta|X)$
        - find the theta that maximizes this function


        t is the index of samples
        h is the index of the z layer